{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a16bc679",
   "metadata": {},
   "source": [
    "Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ac814304",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"Datasets/data.txt\"\n",
    "\n",
    "texts = []\n",
    "\n",
    "with(open(data_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f):\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            texts.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "964cef90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I never thought I’d see you again after all these years.',\n",
       "  'Life has strange ways of bringing people back together when least expected.',\n",
       "  'The evidence doesn’t add up. The fingerprints on the weapon belong to someone who wasn’t even at the crime scene that night.',\n",
       "  'We’ve tried every possible treatment, but his condition remains stable. The next few hours will be critical for his full recovery.',\n",
       "  'Your mission is simple: retrieve the stolen data, avoid enemy surveillance, and ensure nobody knows you were ever there.'],\n",
       " 432)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5], len(texts)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c627a",
   "metadata": {},
   "source": [
    "We can See a lot of noise of blank strings...So preprocessing Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5ccc29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9?.!,']+\", \" \", text)\n",
    "    text = re.sub(r\"[()]\", \"\", text)\n",
    "    text = re.sub(r\"\\.{2,}\", \".\", text)\n",
    "    text = re.sub(r\"\\,{2,}\", \",\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c77f1dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "texts = [clean_text(text) for text in texts]\n",
    "random.seed(42)\n",
    "random.shuffle(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0b52bc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i ll notify you if there are any significant updates.',\n",
       " 'sometimes to love someone, you got to be a stranger.',\n",
       " 'i ll make sure to double check the details.',\n",
       " 'hasta la vista, baby this mission is terminated.',\n",
       " 'you re gonna need a bigger team if you want to win this battle.']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5b66a027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1324\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 5000, oov_token=\"<oov>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "14ebaca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f4d9b498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 34, 305, 3, 22, 32, 26, 39, 211, 212],\n",
       " [97, 4, 111, 213, 3, 161, 4, 17, 7, 306],\n",
       " [5, 34, 76, 162, 4, 307, 214, 2, 163],\n",
       " [530, 531, 532, 308, 9, 215, 13, 533],\n",
       " [3, 38, 164, 43, 7, 127, 91, 22, 3, 128, 4, 309, 9, 534],\n",
       " [6, 66, 310, 7, 535, 112, 40, 113, 58],\n",
       " [2,\n",
       "  536,\n",
       "  8,\n",
       "  216,\n",
       "  217,\n",
       "  537,\n",
       "  67,\n",
       "  538,\n",
       "  539,\n",
       "  22,\n",
       "  6,\n",
       "  51,\n",
       "  11,\n",
       "  540,\n",
       "  6,\n",
       "  19,\n",
       "  541,\n",
       "  16,\n",
       "  542,\n",
       "  10,\n",
       "  218,\n",
       "  311,\n",
       "  39,\n",
       "  312],\n",
       " [35, 59, 543, 24, 2, 92, 544],\n",
       " [41, 545, 11, 546, 4, 20, 10, 5, 165, 23, 21, 114, 77, 11],\n",
       " [547,\n",
       "  19,\n",
       "  17,\n",
       "  313,\n",
       "  98,\n",
       "  16,\n",
       "  2,\n",
       "  314,\n",
       "  15,\n",
       "  548,\n",
       "  22,\n",
       "  60,\n",
       "  78,\n",
       "  549,\n",
       "  4,\n",
       "  550,\n",
       "  36,\n",
       "  2,\n",
       "  219]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "95b03f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(sequences):\n",
    "    ngrams = []\n",
    "    for seq in sequences:\n",
    "        for i in range(1, len(seq)):\n",
    "            n_gram_seq = seq[:i + 1]\n",
    "            ngrams.append(n_gram_seq)\n",
    "    \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "70c6b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = get_ngrams(train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "88354eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 34],\n",
       " [5, 34, 305],\n",
       " [5, 34, 305, 3],\n",
       " [5, 34, 305, 3, 22],\n",
       " [5, 34, 305, 3, 22, 32],\n",
       " [5, 34, 305, 3, 22, 32, 26],\n",
       " [5, 34, 305, 3, 22, 32, 26, 39],\n",
       " [5, 34, 305, 3, 22, 32, 26, 39, 211],\n",
       " [5, 34, 305, 3, 22, 32, 26, 39, 211, 212],\n",
       " [97, 4]]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3d733216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max(len(seq) for seq in train_tokens)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "19813a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 160 is a big number so we will keep it to 40\n",
    "# max_length = 40\n",
    "\n",
    "# train_tokens = [token for token in train_tokens if len(token) <= max_length]\n",
    "# val_tokens = [token for token in val_tokens if len(token) <= max_length]\n",
    "# test_tokens = [token for token in test_tokens if len(token) <= max_length]\n",
    "\n",
    "# print(f\"Final train size: {len(train_tokens)}\")\n",
    "# print(f\"Final validation size: {len(val_tokens)}\")\n",
    "# print(f\"Final test size: {len(test_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8d8245ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tokens shape: (4237, 24)\n"
     ]
    }
   ],
   "source": [
    "train_tokens = pad_sequences(train_tokens, maxlen=max_length, padding='pre')\n",
    "print(f\"Train tokens shape: {train_tokens.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "843912a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   5,  34],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   5,  34, 305],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   5,  34, 305,   3],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   5,  34, 305,   3,  22],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   5,  34, 305,   3,  22,  32],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   5,  34, 305,   3,  22,  32,  26],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   5,  34, 305,   3,  22,  32,  26,  39],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   5,  34, 305,   3,  22,  32,  26,  39, 211],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   5,  34, 305,   3,  22,  32,  26,  39, 211, 212],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,  97,   4]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2c9273e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (4237, 23), y_train shape: (4237,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X_train = train_tokens[:, :-1]\n",
    "y_train = train_tokens[:, -1]\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "97570892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   5],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   5,  34],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   5,  34, 305],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   5,  34, 305,   3],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   5,  34, 305,   3,  22]], dtype=int32),\n",
       " array([ 34, 305,   3,  22,  32], dtype=int32))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5], y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd8a6b1",
   "metadata": {},
   "source": [
    "Using GLOVE 6B 100D pretrained model as embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6e085508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set your file path\n",
    "embedding_index = {}\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coeffs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coeffs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embedding_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "df08be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # Random initialization for OOV words (optional)\n",
    "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "20bc5281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1324, 100),\n",
       " array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00],\n",
       "        [ 1.35092033e-02,  8.13459050e-01,  1.26888508e-01,\n",
       "         -4.70906303e-01,  3.84160488e-01,  3.76297097e-01,\n",
       "          8.78681461e-02, -2.35684357e-01, -2.95055363e-01,\n",
       "          6.49086115e-02,  1.76515513e-01, -2.75303488e-01,\n",
       "          1.18638838e-01, -3.09955739e-01,  9.42495934e-01,\n",
       "          4.91160923e-01, -6.27961619e-01,  8.05285778e-01,\n",
       "          1.13598219e+00, -8.19725183e-01, -1.12140855e-01,\n",
       "          9.93314272e-01,  3.78989645e-01, -3.48316837e-01,\n",
       "         -4.73904285e-01, -6.27323500e-01,  5.22228525e-01,\n",
       "          6.12000211e-01,  1.21967330e+00,  2.98091397e-01,\n",
       "          1.12358053e+00, -9.73067439e-01, -1.10327866e+00,\n",
       "         -5.94462184e-01, -4.24882495e-01, -1.21828692e-02,\n",
       "         -1.04472257e+00, -7.46846820e-01,  4.05306626e-01,\n",
       "         -9.39902066e-01,  2.24433817e-01, -6.79307818e-01,\n",
       "         -9.16225423e-01, -5.70098198e-01,  5.17375909e-01,\n",
       "          1.17521972e-01,  6.32059682e-02, -2.21993032e-01,\n",
       "         -7.33648703e-01, -1.25050306e+00, -1.20570379e+00,\n",
       "         -2.97261216e-01, -3.19851490e-01,  2.02147211e-01,\n",
       "         -2.47692031e-01, -1.55271861e-01,  4.93387208e-01,\n",
       "         -9.52868695e-01, -7.33288806e-02,  3.46079729e-01,\n",
       "         -5.03991957e-01, -3.21986571e-01, -1.60801239e-01,\n",
       "          1.25188811e-01,  4.22101430e-01, -1.25430232e-01,\n",
       "          4.24714808e-01,  6.34490845e-01,  4.49943804e-01,\n",
       "          8.13451523e-02,  5.33606631e-01, -1.97736928e-01,\n",
       "          8.55637807e-01, -2.43955412e-01,  7.18762268e-01,\n",
       "         -8.67188321e-01, -9.34056146e-01, -8.15585163e-02,\n",
       "          1.48842627e+00,  6.92831526e-01, -1.90374760e-02,\n",
       "         -1.47027598e+00, -3.43208316e-01, -1.27878053e+00,\n",
       "         -4.41232363e-01, -2.00606699e-01, -1.15313121e+00,\n",
       "          1.09537039e-01, -5.12088269e-01,  4.89793439e-02,\n",
       "          6.66971012e-01,  8.03923483e-01, -8.42351473e-01,\n",
       "          2.20465867e-01,  3.51820832e-01,  9.30473293e-01,\n",
       "          1.29719475e-01,  1.26670259e-01,  8.03583008e-01,\n",
       "         -7.66602991e-03],\n",
       "        [-3.81940007e-02, -2.44870007e-01,  7.28120029e-01,\n",
       "         -3.99610013e-01,  8.31720009e-02,  4.39530015e-02,\n",
       "         -3.91409993e-01,  3.34399998e-01, -5.75450003e-01,\n",
       "          8.74589980e-02,  2.87869990e-01, -6.73099980e-02,\n",
       "          3.09060007e-01, -2.63839990e-01, -1.32310003e-01,\n",
       "         -2.07570001e-01,  3.33950013e-01, -3.38479996e-01,\n",
       "         -3.17429990e-01, -4.83359993e-01,  1.46400005e-01,\n",
       "         -3.73039991e-01,  3.45770001e-01,  5.20410016e-02,\n",
       "          4.49460000e-01, -4.69709992e-01,  2.62800008e-02,\n",
       "         -5.41549981e-01, -1.55180007e-01, -1.41069993e-01,\n",
       "         -3.97219993e-02,  2.82770008e-01,  1.43930003e-01,\n",
       "          2.34640002e-01, -3.10209990e-01,  8.61729980e-02,\n",
       "          2.03970000e-01,  5.26239991e-01,  1.71639994e-01,\n",
       "         -8.23780000e-02, -7.17869997e-01, -4.15309995e-01,\n",
       "          2.03349993e-01, -1.27629995e-01,  4.13670003e-01,\n",
       "          5.51869988e-01,  5.79079986e-01, -3.34769994e-01,\n",
       "         -3.65590006e-01, -5.48569977e-01, -6.28919974e-02,\n",
       "          2.65839994e-01,  3.02049994e-01,  9.97749984e-01,\n",
       "         -8.04809988e-01, -3.02430010e+00,  1.25399996e-02,\n",
       "         -3.69419992e-01,  2.21670008e+00,  7.22010016e-01,\n",
       "         -2.49779999e-01,  9.21360016e-01,  3.45139988e-02,\n",
       "          4.67449993e-01,  1.10790002e+00, -1.93580002e-01,\n",
       "         -7.45749995e-02,  2.33530000e-01, -5.20620011e-02,\n",
       "         -2.20440000e-01,  5.71620017e-02, -1.58059999e-01,\n",
       "         -3.07980001e-01, -4.16249990e-01,  3.79720002e-01,\n",
       "          1.50059998e-01, -5.32119989e-01, -2.05500007e-01,\n",
       "         -1.25259995e+00,  7.16240034e-02,  7.05649972e-01,\n",
       "          4.97440010e-01, -4.20630008e-01,  2.61480004e-01,\n",
       "         -1.53799999e+00, -3.02230000e-01, -7.34380037e-02,\n",
       "         -2.83120006e-01,  3.71039987e-01, -2.52169997e-01,\n",
       "          1.62150003e-02, -1.70990005e-02, -3.89840007e-01,\n",
       "          8.74239981e-01, -7.25690007e-01, -5.10580003e-01,\n",
       "         -5.20280004e-01, -1.45899996e-01,  8.27799976e-01,\n",
       "          2.70619988e-01],\n",
       "        [-4.98860002e-01,  7.66020000e-01,  8.97509992e-01,\n",
       "         -7.85470009e-01, -6.85500026e-01,  6.26089990e-01,\n",
       "         -3.96550000e-01,  3.49130005e-01,  3.33339989e-01,\n",
       "         -4.52329993e-01,  6.12230003e-01,  7.59480000e-02,\n",
       "          2.25309998e-01,  1.63650006e-01,  2.80950010e-01,\n",
       "         -2.47580007e-01,  9.90089960e-03,  7.11080015e-01,\n",
       "         -7.58589983e-01,  8.74230027e-01,  3.10410000e-03,\n",
       "          3.57959986e-01, -3.52329999e-01, -6.65000021e-01,\n",
       "          3.84469986e-01,  6.26770020e-01, -5.15429974e-01,\n",
       "         -9.66530025e-01,  6.15170002e-01, -7.54549980e-01,\n",
       "         -1.23589998e-02,  1.11880004e+00,  3.57190013e-01,\n",
       "          7.17689982e-03,  2.02549994e-01,  5.01100004e-01,\n",
       "         -4.40459996e-01,  1.06610000e-01,  7.93910027e-01,\n",
       "         -8.09480011e-01, -1.56009998e-02, -2.28880003e-01,\n",
       "         -3.41980010e-01, -1.00650001e+00, -8.76299977e-01,\n",
       "          1.51649997e-01, -8.53390023e-02, -6.46499991e-01,\n",
       "         -1.67329997e-01, -1.44990003e+00, -6.59050001e-03,\n",
       "          4.81129996e-03, -1.24450000e-02,  1.04740000e+00,\n",
       "         -1.93810001e-01, -2.59910011e+00,  4.05279994e-01,\n",
       "          4.38030005e-01,  1.93320000e+00,  4.58139986e-01,\n",
       "         -4.88190018e-02,  1.43079996e+00, -7.86390007e-01,\n",
       "         -2.07920000e-01,  1.09000003e+00,  2.48160005e-01,\n",
       "          1.14870000e+00,  5.14810026e-01, -2.18319997e-01,\n",
       "         -4.57199991e-01,  1.38880000e-01, -2.63689995e-01,\n",
       "          1.36470005e-01, -6.05390012e-01,  9.95860025e-02,\n",
       "          2.33439997e-01,  1.36470005e-01, -1.84599996e-01,\n",
       "         -4.77339998e-02, -1.83919996e-01,  5.27190030e-01,\n",
       "         -2.88500011e-01, -1.07420003e+00, -4.67000008e-02,\n",
       "         -1.83019996e+00, -2.11970001e-01,  2.97999997e-02,\n",
       "         -3.09639990e-01, -4.33860004e-01, -3.64630014e-01,\n",
       "         -3.27380002e-01, -9.34270024e-03,  4.72050011e-01,\n",
       "         -5.16910017e-01, -5.91759980e-01, -3.23430002e-01,\n",
       "          2.00519994e-01, -4.11790013e-01,  4.05389994e-01,\n",
       "          7.85040021e-01],\n",
       "        [-1.89700007e-01,  5.00239991e-02,  1.90840006e-01,\n",
       "         -4.91839983e-02, -8.97369981e-02,  2.10060000e-01,\n",
       "         -5.49520016e-01,  9.83769968e-02, -2.01350003e-01,\n",
       "          3.42409998e-01, -9.26769972e-02,  1.60999998e-01,\n",
       "         -1.32679999e-01, -2.81599998e-01,  1.87370002e-01,\n",
       "         -4.29589987e-01,  9.60389972e-01,  1.39719993e-01,\n",
       "         -1.07809997e+00,  4.05180007e-01,  5.05389988e-01,\n",
       "         -5.50639987e-01,  4.84400004e-01,  3.80439997e-01,\n",
       "         -2.90549989e-03, -3.49420011e-01, -9.96960029e-02,\n",
       "         -7.83680022e-01,  1.03629994e+00, -2.31399998e-01,\n",
       "         -4.71210003e-01,  5.71259975e-01, -2.14540005e-01,\n",
       "          3.59580010e-01, -4.83190000e-01,  1.08749998e+00,\n",
       "          2.85239995e-01,  1.24470003e-01, -3.92480008e-02,\n",
       "         -7.67320022e-02, -7.63429999e-01, -3.24090004e-01,\n",
       "         -5.74899971e-01, -1.08930004e+00, -4.18110013e-01,\n",
       "          4.51200008e-01,  1.21119998e-01, -5.13670027e-01,\n",
       "         -1.33489996e-01, -1.13779998e+00, -2.87680000e-01,\n",
       "          1.67740002e-01,  5.58040023e-01,  1.53869998e+00,\n",
       "          1.88590009e-02, -2.97210002e+00, -2.42160007e-01,\n",
       "         -9.24950004e-01,  2.19919991e+00,  2.82339990e-01,\n",
       "         -3.47799987e-01,  5.16210020e-01, -4.33869988e-01,\n",
       "          3.68519992e-01,  7.45729983e-01,  7.21020028e-02,\n",
       "          2.79309988e-01,  9.25689995e-01, -5.03359996e-02,\n",
       "         -8.58560026e-01, -1.35800004e-01, -9.25509989e-01,\n",
       "         -3.39910001e-01, -1.03939998e+00, -6.72030002e-02,\n",
       "         -2.13789999e-01, -4.76900011e-01,  2.13770002e-01,\n",
       "         -8.40080023e-01,  5.25359996e-02,  5.92980027e-01,\n",
       "          2.96039999e-01, -6.76440001e-01,  1.39160007e-01,\n",
       "         -1.55040002e+00, -2.07650006e-01,  7.22199976e-01,\n",
       "          5.20560026e-01, -7.62209967e-02, -1.51940003e-01,\n",
       "         -1.31339997e-01,  5.86169995e-02, -3.18690002e-01,\n",
       "         -6.14189982e-01, -6.23929977e-01, -4.15479988e-01,\n",
       "         -3.81750017e-02, -3.98039997e-01,  4.76469994e-01,\n",
       "         -1.59830004e-01]]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape, embedding_matrix[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cb4254ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_layer (Embedding  (None, 23, 100)           132400    \n",
      " )                                                               \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirecti  (None, 64)                34048     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_layer (Dense)         (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_layer (Dropout)     (None, 64)                0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1324)              86060     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256924 (1003.61 KB)\n",
      "Trainable params: 124396 (485.92 KB)\n",
      "Non-trainable params: 132528 (517.69 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from functools import partial\n",
    "\n",
    "embedding_dim = 100\n",
    "Embedding = partial(Embedding, weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length-1, name='embedding_layer'),\n",
    "    Bidirectional(LSTM(32, name='bidirectional_lstm')),\n",
    "    Dense(64, activation='relu', name='dense_layer'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3, name='dropout_layer'),\n",
    "    Dense(vocab_size, activation='softmax', name='output_layer')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a97205e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from helper_functions import create_tensorboard_callback\n",
    "\n",
    "lr_reduce = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4d5cb449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorboard_logs/glove6B100D_lstm/20250616-025808\n",
      "Epoch 1/50\n",
      "265/265 [==============================] - 5s 11ms/step - loss: 6.9674 - accuracy: 0.0323 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 6.0482 - accuracy: 0.0649 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 5.6719 - accuracy: 0.0876 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 5.3804 - accuracy: 0.1029 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 5.1127 - accuracy: 0.1154 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 4.8429 - accuracy: 0.1296 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 4.6005 - accuracy: 0.1374 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 4.3586 - accuracy: 0.1584 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 4.1139 - accuracy: 0.1754 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 3.9359 - accuracy: 0.1940 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "265/265 [==============================] - 3s 10ms/step - loss: 3.7060 - accuracy: 0.2148 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 3.5458 - accuracy: 0.2346 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 3.3953 - accuracy: 0.2495 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 3.2451 - accuracy: 0.2764 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 3.1027 - accuracy: 0.2993 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 2.9725 - accuracy: 0.3203 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 2.8569 - accuracy: 0.3382 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 2.7501 - accuracy: 0.3616 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 2.6776 - accuracy: 0.3715 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 2.5741 - accuracy: 0.3918 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 2.5062 - accuracy: 0.4095 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 2.4275 - accuracy: 0.4137 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 2.3589 - accuracy: 0.4364 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "265/265 [==============================] - 3s 10ms/step - loss: 2.2818 - accuracy: 0.4437 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 2.2529 - accuracy: 0.4435 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 2.1905 - accuracy: 0.4706 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 2.1240 - accuracy: 0.4793 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 2.0802 - accuracy: 0.4793 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 2.0444 - accuracy: 0.4902 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "265/265 [==============================] - 3s 10ms/step - loss: 1.9923 - accuracy: 0.5018 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 1.9722 - accuracy: 0.5051 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "265/265 [==============================] - 3s 10ms/step - loss: 1.9431 - accuracy: 0.5112 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 1.9007 - accuracy: 0.5242 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "265/265 [==============================] - 3s 10ms/step - loss: 1.8761 - accuracy: 0.5232 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "265/265 [==============================] - 3s 10ms/step - loss: 1.8272 - accuracy: 0.5301 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 1.8231 - accuracy: 0.5275 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "265/265 [==============================] - 3s 10ms/step - loss: 1.7402 - accuracy: 0.5426 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "260/265 [============================>.] - ETA: 0s - loss: 1.7414 - accuracy: 0.5529\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "265/265 [==============================] - 3s 10ms/step - loss: 1.7416 - accuracy: 0.5527 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "265/265 [==============================] - 3s 10ms/step - loss: 1.5800 - accuracy: 0.5945 - lr: 5.0000e-04\n",
      "Epoch 40/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 1.5428 - accuracy: 0.5877 - lr: 5.0000e-04\n",
      "Epoch 41/50\n",
      "265/265 [==============================] - 3s 10ms/step - loss: 1.5116 - accuracy: 0.5974 - lr: 5.0000e-04\n",
      "Epoch 42/50\n",
      "265/265 [==============================] - 3s 10ms/step - loss: 1.4815 - accuracy: 0.6148 - lr: 5.0000e-04\n",
      "Epoch 43/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 1.4715 - accuracy: 0.6115 - lr: 5.0000e-04\n",
      "Epoch 44/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 1.4302 - accuracy: 0.6205 - lr: 5.0000e-04\n",
      "Epoch 45/50\n",
      "265/265 [==============================] - ETA: 0s - loss: 1.4342 - accuracy: 0.6236\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 1.4342 - accuracy: 0.6236 - lr: 5.0000e-04\n",
      "Epoch 46/50\n",
      "265/265 [==============================] - 3s 10ms/step - loss: 1.3513 - accuracy: 0.6441 - lr: 2.5000e-04\n",
      "Epoch 47/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 1.3371 - accuracy: 0.6483 - lr: 2.5000e-04\n",
      "Epoch 48/50\n",
      "265/265 [==============================] - 3s 10ms/step - loss: 1.3072 - accuracy: 0.6472 - lr: 2.5000e-04\n",
      "Epoch 49/50\n",
      "265/265 [==============================] - 3s 11ms/step - loss: 1.2785 - accuracy: 0.6587 - lr: 2.5000e-04\n",
      "Epoch 50/50\n",
      "261/265 [============================>.] - ETA: 0s - loss: 1.2889 - accuracy: 0.6523\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "265/265 [==============================] - 3s 10ms/step - loss: 1.2924 - accuracy: 0.6514 - lr: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=16,\n",
    "                    callbacks=[create_tensorboard_callback(\"tensorboard_logs\", \"glove6B100D_lstm\"), lr_reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ae3c4f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Lambda\n",
    "\n",
    "preprocessing_steps = Sequential([\n",
    "    Lambda(lambda x: clean_text(x)),  # Remove URLs\n",
    "    Lambda(lambda x: tokenizer.texts_to_sequences([x])[0]),  # Tokenize the text\n",
    "    Lambda(lambda x: pad_sequences([x], maxlen=max_length-1, padding='pre')[0]),  # Pad the sequences\n",
    "    Lambda(lambda x: tf.expand_dims(x, axis=0))  # Add batch dimension\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "93c413ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = Sequential([\n",
    "    preprocessing_steps,\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b33492a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: First rule of the fight club\n",
      "Generated: First rule of the fight club is you do not talk about fight club\n",
      "\n",
      "--------------------------------------------------\n",
      "Input: Good morning\n",
      "Generated: Good morning i hope your lives extraordinary lives extraordinary lives\n",
      "\n",
      "--------------------------------------------------\n",
      "Input: Please\n",
      "Generated: Please let me know if you foresee any potential\n",
      "\n",
      "--------------------------------------------------\n",
      "Input: I will make him\n",
      "Generated: I will make him to offer but blood toil tears and sweat\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "texts = [\"First rule of the fight club\", \"Good morning\", \"Please\", \"I will make him\"]\n",
    "\n",
    "for text in texts:\n",
    "    text_copy = text\n",
    "    for _ in range(8):\n",
    "        result = final_model(text_copy)\n",
    "        next_word_index = np.argmax(result[0])\n",
    "        next_word = tokenizer.index_word.get(next_word_index, \"\")\n",
    "        if not next_word:\n",
    "            break\n",
    "        text_copy += \" \" + next_word\n",
    "    print(f\"Input: {text}\\nGenerated: {text_copy}\\n\")\n",
    "    print(\"-\" * 50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6f857d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajat/miniconda3/envs/tf14/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Model/glove6B100D_lstm.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
