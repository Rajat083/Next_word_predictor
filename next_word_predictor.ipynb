{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a16bc679",
   "metadata": {},
   "source": [
    "Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac814304",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"Datasets/data.txt\"\n",
    "\n",
    "texts = []\n",
    "\n",
    "with(open(data_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f):\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            texts.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964cef90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I never thought I’d see you again after all these years.',\n",
       "  'Life has strange ways of bringing people back together when least expected.',\n",
       "  'The evidence doesn’t add up. The fingerprints on the weapon belong to someone who wasn’t even at the crime scene that night.',\n",
       "  'We’ve tried every possible treatment, but his condition remains stable. The next few hours will be critical for his full recovery.',\n",
       "  'Your mission is simple: retrieve the stolen data, avoid enemy surveillance, and ensure nobody knows you were ever there.'],\n",
       " 552)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5], len(texts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e72140db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c627a",
   "metadata": {},
   "source": [
    "We can See a lot of noise of blank strings...So preprocessing Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ccc29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9?.!,']+\", \" \", text)\n",
    "    text = re.sub(r\"[()]\", \"\", text)\n",
    "    text = re.sub(r\"\\.{2,}\", \".\", text)\n",
    "    text = re.sub(r\"\\,{2,}\", \",\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c77f1dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "texts = [clean_text(text) for text in texts]\n",
    "random.seed(42)\n",
    "random.shuffle(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b52bc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i really appreciate your help with this.',\n",
       " 'i heard there s a new caf opening downtown next weekend.',\n",
       " 'do you want to split the tasks to get them done faster?',\n",
       " 'it s just a flesh wound.',\n",
       " 'could you help me carry these boxes upstairs?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b66a027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 15:31:33.632095: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-16 15:31:33.911736: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-16 15:31:33.911788: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-16 15:31:33.913754: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-16 15:31:34.027711: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1502\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 5000, oov_token=\"<oov>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14ebaca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4d9b498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 62, 110, 12, 111, 21, 10],\n",
       " [5, 168, 37, 8, 7, 66, 633, 634, 386, 63, 76],\n",
       " [34, 3, 77, 4, 267, 2, 268, 4, 49, 143, 269, 270],\n",
       " [9, 8, 59, 7, 635, 636],\n",
       " [64, 3, 111, 17, 387, 144, 637, 638],\n",
       " [34, 3, 23, 42, 271, 13, 2, 76],\n",
       " [2, 388, 639, 100, 35, 10, 640, 6, 44, 641, 642, 643, 60],\n",
       " [5, 27, 389, 3, 24, 37, 26, 42, 272, 201],\n",
       " [6, 390, 38, 391, 644],\n",
       " [2, 202, 15, 645, 646, 3]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95b03f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(sequences):\n",
    "    ngrams = []\n",
    "    for seq in sequences:\n",
    "        for i in range(1, len(seq)):\n",
    "            n_gram_seq = seq[:i + 1]\n",
    "            ngrams.append(n_gram_seq)\n",
    "    \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70c6b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = get_ngrams(train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88354eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 62],\n",
       " [5, 62, 110],\n",
       " [5, 62, 110, 12],\n",
       " [5, 62, 110, 12, 111],\n",
       " [5, 62, 110, 12, 111, 21],\n",
       " [5, 62, 110, 12, 111, 21, 10],\n",
       " [5, 168],\n",
       " [5, 168, 37],\n",
       " [5, 168, 37, 8],\n",
       " [5, 168, 37, 8, 7]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d733216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max(len(seq) for seq in train_tokens)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19813a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 160 is a big number so we will keep it to 40\n",
    "# max_length = 40\n",
    "\n",
    "# train_tokens = [token for token in train_tokens if len(token) <= max_length]\n",
    "# val_tokens = [token for token in val_tokens if len(token) <= max_length]\n",
    "# test_tokens = [token for token in test_tokens if len(token) <= max_length]\n",
    "\n",
    "# print(f\"Final train size: {len(train_tokens)}\")\n",
    "# print(f\"Final validation size: {len(val_tokens)}\")\n",
    "# print(f\"Final test size: {len(test_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d8245ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tokens shape: (5433, 24)\n"
     ]
    }
   ],
   "source": [
    "train_tokens = pad_sequences(train_tokens, maxlen=max_length, padding='pre')\n",
    "print(f\"Train tokens shape: {train_tokens.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "843912a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   5,  62],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   5,  62, 110],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   5,  62, 110,  12],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   5,  62, 110,  12, 111],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   5,  62, 110,  12, 111,  21],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   5,  62, 110,  12, 111,  21,  10],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   5, 168],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   5, 168,  37],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   5, 168,  37,   8],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   5, 168,  37,   8,   7]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c9273e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5433, 23), y_train shape: (5433,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X_train = train_tokens[:, :-1]\n",
    "y_train = train_tokens[:, -1]\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97570892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   5],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   5,  62],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   5,  62, 110],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   5,  62, 110,  12],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   5,  62, 110,  12, 111]], dtype=int32),\n",
       " array([ 62, 110,  12, 111,  21], dtype=int32))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5], y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd8a6b1",
   "metadata": {},
   "source": [
    "Using GLOVE 6B 100D pretrained model as embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e085508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set your file path\n",
    "embedding_index = {}\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coeffs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coeffs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embedding_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df08be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # Random initialization for OOV words (optional)\n",
    "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20bc5281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1502, 100),\n",
       " array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00],\n",
       "        [-3.17455564e-01, -1.03692245e-01,  7.12275651e-01,\n",
       "         -9.15897532e-01, -6.96567741e-01,  9.41944911e-02,\n",
       "          6.38705713e-02,  3.81066453e-01,  2.17979006e-02,\n",
       "         -8.20431816e-01,  3.77067967e-01,  6.12610922e-01,\n",
       "          1.15268075e+00, -7.33135908e-02, -3.40867497e-01,\n",
       "         -1.31495951e-01,  7.28425352e-01,  2.34487827e-01,\n",
       "         -5.53978928e-01, -9.33291997e-02,  1.19899414e+00,\n",
       "         -1.02654767e-01,  5.22998932e-01,  6.45708820e-02,\n",
       "         -3.79104515e-01,  8.17581992e-02, -1.05089159e+00,\n",
       "          1.52171113e+00,  1.06913589e+00, -1.03106292e+00,\n",
       "          5.30312645e-01,  1.71533452e-02, -4.76260807e-01,\n",
       "          6.91891717e-01, -6.68000007e-01, -5.76812431e-01,\n",
       "          2.98132899e-01, -4.83598837e-01, -9.34047106e-02,\n",
       "          5.16532838e-01, -5.83212007e-01, -2.25263570e-01,\n",
       "          5.76795083e-01, -3.32491527e-01,  9.48540535e-01,\n",
       "          6.85470116e-01,  3.38291015e-01, -2.69689607e-01,\n",
       "          1.20918962e-01,  1.21407949e+00, -2.86177469e-01,\n",
       "         -1.00238495e+00,  2.35501299e-01,  2.69865302e-01,\n",
       "          2.16646989e-01, -2.98264790e-01, -1.32331352e+00,\n",
       "         -5.38309299e-01, -7.97820784e-01,  3.62485072e-01,\n",
       "          4.32707850e-02, -6.41271447e-01, -6.45158988e-01,\n",
       "         -1.15053463e-01, -7.62896078e-01, -5.64881511e-01,\n",
       "          4.92924352e-01, -6.39710754e-02,  1.36396173e-01,\n",
       "          1.39202834e+00,  2.27888714e-01,  9.40298605e-02,\n",
       "          7.15087697e-01,  1.25099422e+00,  4.03951509e-01,\n",
       "         -8.51230801e-01, -1.93863276e-01, -8.92895329e-01,\n",
       "          7.66102937e-01, -1.01208225e+00, -2.45808575e-01,\n",
       "          1.31806408e-01,  4.33334205e-01,  6.66510374e-01,\n",
       "          5.18551510e-01, -4.04231680e-01, -4.50195819e-01,\n",
       "         -2.18410031e-02,  5.28759145e-01, -1.42565529e+00,\n",
       "          4.97418697e-01,  5.36970429e-01,  5.50664661e-01,\n",
       "          5.05291329e-01,  9.76542215e-02, -5.09632451e-01,\n",
       "         -2.12621869e-01,  1.10775298e+00,  8.89821333e-01,\n",
       "         -9.54674869e-01],\n",
       "        [-3.81940007e-02, -2.44870007e-01,  7.28120029e-01,\n",
       "         -3.99610013e-01,  8.31720009e-02,  4.39530015e-02,\n",
       "         -3.91409993e-01,  3.34399998e-01, -5.75450003e-01,\n",
       "          8.74589980e-02,  2.87869990e-01, -6.73099980e-02,\n",
       "          3.09060007e-01, -2.63839990e-01, -1.32310003e-01,\n",
       "         -2.07570001e-01,  3.33950013e-01, -3.38479996e-01,\n",
       "         -3.17429990e-01, -4.83359993e-01,  1.46400005e-01,\n",
       "         -3.73039991e-01,  3.45770001e-01,  5.20410016e-02,\n",
       "          4.49460000e-01, -4.69709992e-01,  2.62800008e-02,\n",
       "         -5.41549981e-01, -1.55180007e-01, -1.41069993e-01,\n",
       "         -3.97219993e-02,  2.82770008e-01,  1.43930003e-01,\n",
       "          2.34640002e-01, -3.10209990e-01,  8.61729980e-02,\n",
       "          2.03970000e-01,  5.26239991e-01,  1.71639994e-01,\n",
       "         -8.23780000e-02, -7.17869997e-01, -4.15309995e-01,\n",
       "          2.03349993e-01, -1.27629995e-01,  4.13670003e-01,\n",
       "          5.51869988e-01,  5.79079986e-01, -3.34769994e-01,\n",
       "         -3.65590006e-01, -5.48569977e-01, -6.28919974e-02,\n",
       "          2.65839994e-01,  3.02049994e-01,  9.97749984e-01,\n",
       "         -8.04809988e-01, -3.02430010e+00,  1.25399996e-02,\n",
       "         -3.69419992e-01,  2.21670008e+00,  7.22010016e-01,\n",
       "         -2.49779999e-01,  9.21360016e-01,  3.45139988e-02,\n",
       "          4.67449993e-01,  1.10790002e+00, -1.93580002e-01,\n",
       "         -7.45749995e-02,  2.33530000e-01, -5.20620011e-02,\n",
       "         -2.20440000e-01,  5.71620017e-02, -1.58059999e-01,\n",
       "         -3.07980001e-01, -4.16249990e-01,  3.79720002e-01,\n",
       "          1.50059998e-01, -5.32119989e-01, -2.05500007e-01,\n",
       "         -1.25259995e+00,  7.16240034e-02,  7.05649972e-01,\n",
       "          4.97440010e-01, -4.20630008e-01,  2.61480004e-01,\n",
       "         -1.53799999e+00, -3.02230000e-01, -7.34380037e-02,\n",
       "         -2.83120006e-01,  3.71039987e-01, -2.52169997e-01,\n",
       "          1.62150003e-02, -1.70990005e-02, -3.89840007e-01,\n",
       "          8.74239981e-01, -7.25690007e-01, -5.10580003e-01,\n",
       "         -5.20280004e-01, -1.45899996e-01,  8.27799976e-01,\n",
       "          2.70619988e-01],\n",
       "        [-4.98860002e-01,  7.66020000e-01,  8.97509992e-01,\n",
       "         -7.85470009e-01, -6.85500026e-01,  6.26089990e-01,\n",
       "         -3.96550000e-01,  3.49130005e-01,  3.33339989e-01,\n",
       "         -4.52329993e-01,  6.12230003e-01,  7.59480000e-02,\n",
       "          2.25309998e-01,  1.63650006e-01,  2.80950010e-01,\n",
       "         -2.47580007e-01,  9.90089960e-03,  7.11080015e-01,\n",
       "         -7.58589983e-01,  8.74230027e-01,  3.10410000e-03,\n",
       "          3.57959986e-01, -3.52329999e-01, -6.65000021e-01,\n",
       "          3.84469986e-01,  6.26770020e-01, -5.15429974e-01,\n",
       "         -9.66530025e-01,  6.15170002e-01, -7.54549980e-01,\n",
       "         -1.23589998e-02,  1.11880004e+00,  3.57190013e-01,\n",
       "          7.17689982e-03,  2.02549994e-01,  5.01100004e-01,\n",
       "         -4.40459996e-01,  1.06610000e-01,  7.93910027e-01,\n",
       "         -8.09480011e-01, -1.56009998e-02, -2.28880003e-01,\n",
       "         -3.41980010e-01, -1.00650001e+00, -8.76299977e-01,\n",
       "          1.51649997e-01, -8.53390023e-02, -6.46499991e-01,\n",
       "         -1.67329997e-01, -1.44990003e+00, -6.59050001e-03,\n",
       "          4.81129996e-03, -1.24450000e-02,  1.04740000e+00,\n",
       "         -1.93810001e-01, -2.59910011e+00,  4.05279994e-01,\n",
       "          4.38030005e-01,  1.93320000e+00,  4.58139986e-01,\n",
       "         -4.88190018e-02,  1.43079996e+00, -7.86390007e-01,\n",
       "         -2.07920000e-01,  1.09000003e+00,  2.48160005e-01,\n",
       "          1.14870000e+00,  5.14810026e-01, -2.18319997e-01,\n",
       "         -4.57199991e-01,  1.38880000e-01, -2.63689995e-01,\n",
       "          1.36470005e-01, -6.05390012e-01,  9.95860025e-02,\n",
       "          2.33439997e-01,  1.36470005e-01, -1.84599996e-01,\n",
       "         -4.77339998e-02, -1.83919996e-01,  5.27190030e-01,\n",
       "         -2.88500011e-01, -1.07420003e+00, -4.67000008e-02,\n",
       "         -1.83019996e+00, -2.11970001e-01,  2.97999997e-02,\n",
       "         -3.09639990e-01, -4.33860004e-01, -3.64630014e-01,\n",
       "         -3.27380002e-01, -9.34270024e-03,  4.72050011e-01,\n",
       "         -5.16910017e-01, -5.91759980e-01, -3.23430002e-01,\n",
       "          2.00519994e-01, -4.11790013e-01,  4.05389994e-01,\n",
       "          7.85040021e-01],\n",
       "        [-1.89700007e-01,  5.00239991e-02,  1.90840006e-01,\n",
       "         -4.91839983e-02, -8.97369981e-02,  2.10060000e-01,\n",
       "         -5.49520016e-01,  9.83769968e-02, -2.01350003e-01,\n",
       "          3.42409998e-01, -9.26769972e-02,  1.60999998e-01,\n",
       "         -1.32679999e-01, -2.81599998e-01,  1.87370002e-01,\n",
       "         -4.29589987e-01,  9.60389972e-01,  1.39719993e-01,\n",
       "         -1.07809997e+00,  4.05180007e-01,  5.05389988e-01,\n",
       "         -5.50639987e-01,  4.84400004e-01,  3.80439997e-01,\n",
       "         -2.90549989e-03, -3.49420011e-01, -9.96960029e-02,\n",
       "         -7.83680022e-01,  1.03629994e+00, -2.31399998e-01,\n",
       "         -4.71210003e-01,  5.71259975e-01, -2.14540005e-01,\n",
       "          3.59580010e-01, -4.83190000e-01,  1.08749998e+00,\n",
       "          2.85239995e-01,  1.24470003e-01, -3.92480008e-02,\n",
       "         -7.67320022e-02, -7.63429999e-01, -3.24090004e-01,\n",
       "         -5.74899971e-01, -1.08930004e+00, -4.18110013e-01,\n",
       "          4.51200008e-01,  1.21119998e-01, -5.13670027e-01,\n",
       "         -1.33489996e-01, -1.13779998e+00, -2.87680000e-01,\n",
       "          1.67740002e-01,  5.58040023e-01,  1.53869998e+00,\n",
       "          1.88590009e-02, -2.97210002e+00, -2.42160007e-01,\n",
       "         -9.24950004e-01,  2.19919991e+00,  2.82339990e-01,\n",
       "         -3.47799987e-01,  5.16210020e-01, -4.33869988e-01,\n",
       "          3.68519992e-01,  7.45729983e-01,  7.21020028e-02,\n",
       "          2.79309988e-01,  9.25689995e-01, -5.03359996e-02,\n",
       "         -8.58560026e-01, -1.35800004e-01, -9.25509989e-01,\n",
       "         -3.39910001e-01, -1.03939998e+00, -6.72030002e-02,\n",
       "         -2.13789999e-01, -4.76900011e-01,  2.13770002e-01,\n",
       "         -8.40080023e-01,  5.25359996e-02,  5.92980027e-01,\n",
       "          2.96039999e-01, -6.76440001e-01,  1.39160007e-01,\n",
       "         -1.55040002e+00, -2.07650006e-01,  7.22199976e-01,\n",
       "          5.20560026e-01, -7.62209967e-02, -1.51940003e-01,\n",
       "         -1.31339997e-01,  5.86169995e-02, -3.18690002e-01,\n",
       "         -6.14189982e-01, -6.23929977e-01, -4.15479988e-01,\n",
       "         -3.81750017e-02, -3.98039997e-01,  4.76469994e-01,\n",
       "         -1.59830004e-01]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape, embedding_matrix[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb4254ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 15:31:58.437205: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-06-16 15:31:58.507874: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-06-16 15:31:58.507911: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-06-16 15:31:58.513562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-06-16 15:31:58.513598: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-06-16 15:31:58.513610: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-06-16 15:31:58.754085: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-06-16 15:31:58.754147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-06-16 15:31:58.754152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-06-16 15:31:58.754172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-06-16 15:31:58.754219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2025-06-16 15:31:59.170297: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_layer (Embedding  (None, 23, 100)           150200    \n",
      " )                                                               \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 128)               84480     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense_layer (Dense)         (None, 128)               16512     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 128)               512       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout_layer (Dropout)     (None, 128)               0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1502)              193758    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 445462 (1.70 MB)\n",
      "Trainable params: 295006 (1.13 MB)\n",
      "Non-trainable params: 150456 (587.72 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from functools import partial\n",
    "\n",
    "embedding_dim = 100\n",
    "Embedding = partial(Embedding, weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length-1, name='embedding_layer'),\n",
    "    Bidirectional(LSTM(64, name='bidirectional_lstm')),\n",
    "    Dense(128, activation='relu', name='dense_layer'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3, name='dropout_layer'),\n",
    "    Dense(vocab_size, activation='softmax', name='output_layer')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a97205e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from helper_functions import create_tensorboard_callback\n",
    "\n",
    "lr_reduce = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d5cb449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorboard_logs/glove6B100D_lstm/20250616-153207\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 15:32:09.867518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600\n",
      "2025-06-16 15:32:10.746077: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f84140740c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-06-16 15:32:10.746129: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2025-06-16 15:32:10.752509: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-06-16 15:32:10.835308: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - 7s 10ms/step - loss: 6.7018 - accuracy: 0.0499 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 5.7067 - accuracy: 0.0930 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "340/340 [==============================] - 4s 10ms/step - loss: 5.2650 - accuracy: 0.1211 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "340/340 [==============================] - 4s 10ms/step - loss: 4.8521 - accuracy: 0.1449 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "340/340 [==============================] - 4s 11ms/step - loss: 4.4588 - accuracy: 0.1616 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "340/340 [==============================] - 5s 14ms/step - loss: 4.0625 - accuracy: 0.1925 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "340/340 [==============================] - 4s 10ms/step - loss: 3.7157 - accuracy: 0.2251 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 3.3950 - accuracy: 0.2639 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 3.1046 - accuracy: 0.3019 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 2.8591 - accuracy: 0.3508 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 2.6207 - accuracy: 0.3836 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "340/340 [==============================] - 4s 11ms/step - loss: 2.4335 - accuracy: 0.4252 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 2.2802 - accuracy: 0.4511 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "340/340 [==============================] - 4s 10ms/step - loss: 2.1064 - accuracy: 0.4927 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "340/340 [==============================] - 5s 14ms/step - loss: 1.9972 - accuracy: 0.5084 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "340/340 [==============================] - 4s 10ms/step - loss: 1.8910 - accuracy: 0.5323 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.8024 - accuracy: 0.5476 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.7285 - accuracy: 0.5608 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.6279 - accuracy: 0.5859 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.5759 - accuracy: 0.5967 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.5044 - accuracy: 0.6105 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.4652 - accuracy: 0.6216 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.4206 - accuracy: 0.6289 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "340/340 [==============================] - 4s 12ms/step - loss: 1.3708 - accuracy: 0.6416 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "340/340 [==============================] - 4s 10ms/step - loss: 1.3229 - accuracy: 0.6459 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.2814 - accuracy: 0.6724 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.2252 - accuracy: 0.6738 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1765 - accuracy: 0.6853 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1655 - accuracy: 0.6923 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1447 - accuracy: 0.6974 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "340/340 [==============================] - 4s 10ms/step - loss: 1.1424 - accuracy: 0.6983 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.0968 - accuracy: 0.7035 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "340/340 [==============================] - 4s 13ms/step - loss: 1.0517 - accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "338/340 [============================>.] - ETA: 0s - loss: 1.0703 - accuracy: 0.7151\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.0722 - accuracy: 0.7143 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.9044 - accuracy: 0.7539 - lr: 5.0000e-04\n",
      "Epoch 36/50\n",
      "340/340 [==============================] - 4s 10ms/step - loss: 0.8290 - accuracy: 0.7760 - lr: 5.0000e-04\n",
      "Epoch 37/50\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.8307 - accuracy: 0.7734\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "340/340 [==============================] - 4s 10ms/step - loss: 0.8301 - accuracy: 0.7736 - lr: 5.0000e-04\n",
      "Epoch 38/50\n",
      "340/340 [==============================] - 4s 11ms/step - loss: 0.7321 - accuracy: 0.8029 - lr: 2.5000e-04\n",
      "Epoch 39/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.7237 - accuracy: 0.8001 - lr: 2.5000e-04\n",
      "Epoch 40/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.7001 - accuracy: 0.8056 - lr: 2.5000e-04\n",
      "Epoch 41/50\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.7310 - accuracy: 0.7979\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "340/340 [==============================] - 4s 10ms/step - loss: 0.7310 - accuracy: 0.7979 - lr: 2.5000e-04\n",
      "Epoch 42/50\n",
      "340/340 [==============================] - 4s 11ms/step - loss: 0.6832 - accuracy: 0.8093 - lr: 1.2500e-04\n",
      "Epoch 43/50\n",
      "340/340 [==============================] - 4s 13ms/step - loss: 0.6714 - accuracy: 0.8139 - lr: 1.2500e-04\n",
      "Epoch 44/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.6547 - accuracy: 0.8209 - lr: 1.2500e-04\n",
      "Epoch 45/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.6456 - accuracy: 0.8235 - lr: 1.2500e-04\n",
      "Epoch 46/50\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6484 - accuracy: 0.8224\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "340/340 [==============================] - 4s 10ms/step - loss: 0.6484 - accuracy: 0.8224 - lr: 1.2500e-04\n",
      "Epoch 47/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.6327 - accuracy: 0.8323 - lr: 6.2500e-05\n",
      "Epoch 48/50\n",
      "340/340 [==============================] - 4s 10ms/step - loss: 0.6256 - accuracy: 0.8299 - lr: 6.2500e-05\n",
      "Epoch 49/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.6238 - accuracy: 0.8272 - lr: 6.2500e-05\n",
      "Epoch 50/50\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.6119 - accuracy: 0.8279 - lr: 6.2500e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=16,\n",
    "                    callbacks=[create_tensorboard_callback(\"tensorboard_logs\", \"glove6B100D_lstm\"), lr_reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae3c4f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Lambda\n",
    "\n",
    "preprocessing_steps = Sequential([\n",
    "    Lambda(lambda x: clean_text(x)),  # Remove URLs\n",
    "    Lambda(lambda x: tokenizer.texts_to_sequences([x])[0]),  # Tokenize the text\n",
    "    Lambda(lambda x: pad_sequences([x], maxlen=max_length-1, padding='pre')[0]),  # Pad the sequences\n",
    "    Lambda(lambda x: tf.expand_dims(x, axis=0))  # Add batch dimension\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93c413ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = Sequential([\n",
    "    preprocessing_steps,\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b33492a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: First rule of the fight club\n",
      "Generated: First rule of the fight club is you do not talk about fight club\n",
      "\n",
      "--------------------------------------------------\n",
      "Input: Good morning\n",
      "Generated: Good morning i hope you re doing well today for\n",
      "\n",
      "--------------------------------------------------\n",
      "Input: Please\n",
      "Generated: Please let me know if you foresee any potential\n",
      "\n",
      "--------------------------------------------------\n",
      "Input: I will make\n",
      "Generated: I will make sure to have i cover it this time\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "texts = [\"First rule of the fight club\", \"Good morning\", \"Please\", \"I will make\"]\n",
    "\n",
    "for text in texts:\n",
    "    text_copy = text\n",
    "    for _ in range(8):\n",
    "        result = final_model(text_copy)\n",
    "        next_word_index = np.argmax(result[0])\n",
    "        next_word = tokenizer.index_word.get(next_word_index, \"\")\n",
    "        if not next_word:\n",
    "            break\n",
    "        text_copy += \" \" + next_word\n",
    "    print(f\"Input: {text}\\nGenerated: {text_copy}\\n\")\n",
    "    print(\"-\" * 50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f857d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajat/miniconda3/envs/tf14/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Model/glove6B100D_lstm.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
